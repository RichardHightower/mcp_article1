metadata:
  version: "1.0.0"
  description: "Example: Building AI Integrations with the Model Context Protocol (MCP)"
  timestamp: "2024-01-20T10:30:00"
  generator: "yaml-project"
  generator_version: "0.1.0"
  author: "MCP: Building AI Integrations with the Model Context Protocol"
  tags:
    - "example"
    - "mcp"
    - "ai-integration"
    - "fastmcp"
    - "customer-service"

content:
  files:
    "README.md":
      content: |
        # MCP Customer Service Assistant: Building AI Integrations with the Model Context Protocol

        This project contains working examples for building AI integrations using the Model Context Protocol (MCP).

        ## Overview

        Learn how to build standardized AI integrations that work across multiple platforms using MCP.

        - Build MCP servers with FastMCP framework
        - Create resources, tools, and prompts for AI models
        - Integrate with Claude Desktop, OpenAI, Anthropic, LangChain, DSPy, and LiteLLM
        - Implement async operations for high performance
        - Use Pydantic for data validation and type safety

        ## Prerequisites

        - Python 3.12.9 (managed via pyenv)
        - Poetry for dependency management
        - Go Task for build automation
        - API key for OpenAI or Anthropic (Claude) OR Ollama installed locally

        ## Setup

        1. Clone this repository
        2. Copy `.env.example` to `.env` and configure your LLM provider:
            
            ```bash
            cp .env.example .env
            ```
            
        3. Edit `.env` to select your provider and model:
            - For OpenAI: Set `LLM_PROVIDER=openai` and add your API key
            - For Claude: Set `LLM_PROVIDER=anthropic` and add your API key
            - For Ollama: Set `LLM_PROVIDER=ollama` (install Ollama and pull phi3 model first)
        4. Run the setup task:
            
            ```bash
            task setup
            ```

        ## Supported LLM Providers

        ### OpenAI

        - Model: gpt-4.1-2025-04-14
        - Requires: OpenAI API key

        ### Anthropic (Claude)

        - Model: claude-sonnet-4-20250514
        - Requires: Anthropic API key

        ### Ollama (Local)

        - Model: gemma3:27b
        - Requires: Ollama installed and gemma3:27b model pulled
        - Install: `brew install ollama` (macOS) or see [ollama.ai](https://ollama.ai/)
        - Pull model: `ollama pull gemma3:27b`

        ## Project Structure

        ```
        .
        ‚îú‚îÄ‚îÄ src/
        ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
        ‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # LLM configuration
        ‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # MCP server implementation
        ‚îÇ   ‚îú‚îÄ‚îÄ openai_integration.py     # OpenAI MCP integration
        ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_integration.py  # Anthropic MCP integration
        ‚îÇ   ‚îú‚îÄ‚îÄ langchain_integration.py  # LangChain MCP integration
        ‚îÇ   ‚îú‚îÄ‚îÄ dspy_integration.py       # DSPy MCP integration
        ‚îÇ   ‚îî‚îÄ‚îÄ litellm_integration.py    # LiteLLM MCP integration
        ‚îú‚îÄ‚îÄ tests/
        ‚îÇ   ‚îî‚îÄ‚îÄ test_mcp_server.py        # Unit tests
        ‚îú‚îÄ‚îÄ .env.example                  # Environment template
        ‚îú‚îÄ‚îÄ Taskfile.yml                  # Task automation
        ‚îú‚îÄ‚îÄ server_config.json            # MCP server configuration
        ‚îî‚îÄ‚îÄ pyproject.toml                # Poetry configuration
        ```

        ## Key Concepts Demonstrated

        1. **MCP Architecture**: Three-layer system with hosts, clients, and servers
        2. **Resources**: Standardized data access through custom URI schemes
        3. **Tools**: AI-executable functions for performing actions
        4. **Prompts**: Structured templates for consistent AI behavior
        5. **FastMCP Framework**: Simplified MCP server development with FastAPI
        6. **Multi-Platform Integration**: Connect once, use everywhere

        ## Running Examples

        Run the MCP server:

        ```bash
        task run
        ```

        Or run individual integration examples:

        ```bash
        task run-openai          # OpenAI integration
        task run-anthropic       # Anthropic integration
        task run-langchain       # LangChain integration
        task run-dspy           # DSPy integration
        task run-litellm        # LiteLLM integration
        ```

        Direct Python execution:

        ```bash
        poetry run python src/main.py
        poetry run python src/openai_integration.py
        poetry run python src/anthropic_integration.py
        ```

        ## Available Tasks

        - `task setup` - Set up Python environment and install dependencies
        - `task run` - Run the MCP server
        - `task test` - Run unit tests
        - `task format` - Format code with Black and Ruff
        - `task clean` - Clean up generated files

        ## Virtual Environment Setup Instructions

        ### Prerequisites

        1. Install pyenv (if not already installed):
            
            ```bash
            # macOS
            brew install pyenv
            
            # Linux
            curl https://pyenv.run | bash
            ```
            
        2. Add pyenv to your shell:
            
            ```bash
            # Add to ~/.zshrc or ~/.bashrc
            echo 'export PYENV_ROOT="$HOME/.pyenv"' >> ~/.zshrc
            echo 'command -v pyenv >/dev/null || export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.zshrc
            echo 'eval "$(pyenv init -)"' >> ~/.zshrc
            
            # Reload shell
            source ~/.zshrc
            ```

        ### Setup Steps

        1. **Install Python 3.12.9**:
            
            ```bash
            pyenv install 3.12.9
            ```
            
        2. **Navigate to your project directory**:
            
            ```bash
            cd /path/to/mcp-customer-service
            ```
            
        3. **Set local Python version**:
            
            ```bash
            pyenv local 3.12.9
            ```
            
        4. **Install Poetry** (if not installed):
            
            ```bash
            curl -sSL https://install.python-poetry.org | python3 -
            ```
            
        5. **Install project dependencies**:
            
            ```bash
            poetry install
            ```
            
        6. **Activate the virtual environment**:
            
            ```bash
            poetry config virtualenvs.in-project true
            source .venv/bin/activate
            ```

        ### Alternative: If you have Go Task installed

        Simply run:

        ```bash
        brew install go-task
        task setup
        ```

        ### Configure your LLM provider

        1. **Copy the example env file**:
            
            ```bash
            cp .env.example .env
            ```
            
        2. **Edit .env and set your provider**:
            
            ```bash
            # For OpenAI
            LLM_PROVIDER=openai
            OPENAI_API_KEY=your-key-here
            OPENAI_MODEL=gpt-4.1-2025-04-14
            
            # For Anthropic/Claude
            LLM_PROVIDER=anthropic
            ANTHROPIC_API_KEY=your-key-here
            ANTHROPIC_MODEL=claude-sonnet-4-20250514
            
            # For Ollama (local)
            LLM_PROVIDER=ollama
            OLLAMA_MODEL=gemma3:27b
            # Make sure Ollama is running: ollama serve
            # Pull the model: ollama pull gemma3:27b
            ```

        ### Verify setup

        ```bash
        # Check Python version
        python --version  # Should show 3.12.9

        # Test imports
        python -c "import fastmcp; print('MCP tools installed successfully')"
        ```

        ### Run the example

        ```bash
        poetry run python src/main.py
        ```

        Note: The main.py runs the MCP server, while integration examples demonstrate different client implementations.

        ## Example Output

        The examples demonstrate:

        1. Creating an MCP server with customer service resources and tools
        2. Integrating with multiple AI platforms using the same server
        3. Handling async operations for better performance
        4. Using Pydantic for data validation
        5. Implementing structured prompts for consistent AI responses

        ## Troubleshooting

        - **Ollama connection error**: Make sure Ollama is running (`ollama serve`)
        - **API key errors**: Check your `.env` file has the correct keys
        - **Model not found**: For Ollama, ensure you've pulled the model (`ollama pull gemma3:27b`)
        - **MCP server not starting**: Check the logs for port conflicts or missing dependencies

        ## Learn More

        - [Model Context Protocol Documentation](https://modelcontextprotocol.io)
        - [FastMCP Framework](https://github.com/modelcontextprotocol/fastmcp)
        - [MCP: Building AI Integrations Article](https://example.com/mcp-article)
      metadata:
        extension: ".md"
        language: "markdown"

    "pyproject.toml":
      content: |
        [tool.poetry]
        name = "mcp-customer-service"
        version = "0.1.0"
        description = "MCP Customer Service Assistant - Building AI Integrations with the Model Context Protocol"
        authors = ["Your Name <you@example.com>"]
        readme = "README.md"
        python = "^3.12"

        [tool.poetry.dependencies]
        python = "^3.12"
        fastmcp = "^0.1.0"
        pydantic = "^2.5.0"
        python-dotenv = "^1.0.0"
        openai = "^1.0.0"
        anthropic = "^0.25.0"
        langchain = "^0.1.0"
        langchain-openai = "^0.0.5"
        langchain-mcp-adapters = "^0.1.0"
        dspy-ai = "^2.4.0"
        litellm = "^1.0.0"
        asyncio = "^3.4.3"

        [tool.poetry.group.dev.dependencies]
        pytest = "^7.4.0"
        pytest-asyncio = "^0.21.0"
        black = "^23.0.0"
        ruff = "^0.1.0"
        isort = "^5.12.0"

        [build-system]
        requires = ["poetry-core"]
        build-backend = "poetry.core.masonry.api"

        [tool.black]
        line-length = 88
        target-version = ['py312']

        [tool.ruff]
        line-length = 88
        select = ["E", "F", "I", "N", "W"]
        ignore = ["E501"]

        [tool.pytest.ini_options]
        testpaths = ["tests"]
        asyncio_mode = "auto"
      metadata:
        extension: ".toml"
        language: "toml"

    "Taskfile.yml":
      content: |
        version: '3'

        tasks:
          default:
            desc: "Default task - runs all tasks"
            cmds:
              - task: all

          setup:
            desc: "Set up the Python environment"
            cmds:
              - pyenv install -s 3.12.9
              - pyenv local 3.12.9
              - poetry install
              - poetry config virtualenvs.in-project true
              - chmod +x .venv/bin/activate
              - source .venv/bin/activate

          run:
            desc: "Run the MCP server"
            cmds:
              - poetry run python src/main.py

          run-openai:
            desc: "Run OpenAI integration example"
            cmds:
              - poetry run python src/openai_integration.py

          run-anthropic:
            desc: "Run Anthropic integration example"
            cmds:
              - poetry run python src/anthropic_integration.py

          run-langchain:
            desc: "Run LangChain integration example"
            cmds:
              - poetry run python src/langchain_integration.py

          run-dspy:
            desc: "Run DSPy integration example"
            cmds:
              - poetry run python src/dspy_integration.py

          run-litellm:
            desc: "Run LiteLLM integration example"
            cmds:
              - poetry run python src/litellm_integration.py

          test:
            desc: "Run tests"
            cmds:
              - poetry run pytest tests/ -v

          format:
            desc: "Format code"
            cmds:
              - poetry run black src/ tests/
              - poetry run ruff check --fix src/ tests/
              - poetry run isort src/ tests/

          clean:
            desc: "Clean up generated files"
            cmds:
              - find . -type d -name "__pycache__" -exec rm -rf {} +
              - find . -type f -name "*.pyc" -delete
              - rm -rf .pytest_cache
              - rm -rf .ruff_cache

          all:
            desc: "Run all tasks"
            cmds:
              - task: setup
              - task: format
              - task: test
              - task: run
      metadata:
        extension: ".yml"
        language: "yaml"

    ".env.example":
      content: |
        # LLM Provider Configuration
        # Choose one: openai, anthropic, ollama
        LLM_PROVIDER=openai

        # OpenAI Configuration
        OPENAI_API_KEY=your-openai-api-key-here
        OPENAI_MODEL=gpt-4.1-2025-04-14

        # Anthropic Configuration
        ANTHROPIC_API_KEY=your-anthropic-api-key-here
        ANTHROPIC_MODEL=claude-sonnet-4-20250514

        # Ollama Configuration (Local)
        OLLAMA_MODEL=gemma3:27b
        OLLAMA_BASE_URL=http://localhost:11434

        # MCP Server Configuration
        MCP_SERVER_HOST=localhost
        MCP_SERVER_PORT=8000
      metadata:
        extension: ""
        language: "env"

    "server_config.json":
      content: |
        {
          "mcpServers": {
            "customer-service": {
              "command": "poetry",
              "args": ["run", "python", "src/main.py"]
            }
          }
        }
      metadata:
        extension: ".json"
        language: "json"

    "src/__init__.py":
      content: |
        """MCP Customer Service Assistant Package."""
        
        __version__ = "0.1.0"
      metadata:
        extension: ".py"
        language: "python"

    "src/config.py":
      content: |
        """Configuration module for LLM providers."""
        
        import os
        from typing import Optional
        from dotenv import load_dotenv
        
        # Load environment variables
        load_dotenv()
        
        class Config:
            """Configuration class for LLM providers."""
            
            # LLM Provider
            LLM_PROVIDER: str = os.getenv("LLM_PROVIDER", "openai")
            
            # OpenAI Configuration
            OPENAI_API_KEY: Optional[str] = os.getenv("OPENAI_API_KEY")
            OPENAI_MODEL: str = os.getenv("OPENAI_MODEL", "gpt-4.1-2025-04-14")
            
            # Anthropic Configuration
            ANTHROPIC_API_KEY: Optional[str] = os.getenv("ANTHROPIC_API_KEY")
            ANTHROPIC_MODEL: str = os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-20250514")
            
            # Ollama Configuration
            OLLAMA_MODEL: str = os.getenv("OLLAMA_MODEL", "gemma3:27b")
            OLLAMA_BASE_URL: str = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            
            # MCP Server Configuration
            MCP_SERVER_HOST: str = os.getenv("MCP_SERVER_HOST", "localhost")
            MCP_SERVER_PORT: int = int(os.getenv("MCP_SERVER_PORT", "8000"))
            
            @classmethod
            def validate(cls) -> None:
                """Validate configuration based on selected provider."""
                if cls.LLM_PROVIDER == "openai" and not cls.OPENAI_API_KEY:
                    raise ValueError("OPENAI_API_KEY is required when using OpenAI provider")
                elif cls.LLM_PROVIDER == "anthropic" and not cls.ANTHROPIC_API_KEY:
                    raise ValueError("ANTHROPIC_API_KEY is required when using Anthropic provider")
      metadata:
        extension: ".py"
        language: "python"

    "src/main.py":
      content: |
        import asyncio
        import logging
        from typing import List, Optional
        from datetime import datetime
        from fastmcp import FastMCP
        from pydantic import BaseModel, validator

        # Configure logging for better debugging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        # Initialize FastMCP server
        mcp = FastMCP("Customer Service Assistant")

        # Data models for type safety and validation
        class Customer(BaseModel):
            id: str
            name: str
            email: str
            phone: Optional[str] = None
            account_status: str = "active"
            last_interaction: Optional[datetime] = None

            @validator('email')
            def email_must_be_valid(cls, v):
                if '@' not in v:
                    raise ValueError('Invalid email format')
                return v

        class TicketRequest(BaseModel):
            customer_id: str
            subject: str
            description: str
            priority: str = "normal"

            @validator('priority')
            def priority_must_be_valid(cls, v):
                valid_priorities = ['low', 'normal', 'high', 'urgent']
                if v not in valid_priorities:
                    raise ValueError(
                        'Priority must be: low, normal, high, or urgent'
                    )
                return v

        # Simulated customer database
        CUSTOMERS_DB = {
            "12345": Customer(
                id="12345",
                name="Alice Johnson",
                email="alice@example.com",
                phone="+1-555-0123",
                account_status="active",
                last_interaction=datetime.now()
            ),
            "67890": Customer(
                id="67890",
                name="Bob Smith",
                email="bob@example.com",
                account_status="suspended"
            )
        }

        # MCP Resource: Customer Data Access
        @mcp.resource("customer://{customer_id}")
        async def get_customer_info(customer_id: str) -> Customer:
            """Retrieve customer information by ID."""
            logger.info(f"Retrieving customer info for ID: {customer_id}")

            if customer_id not in CUSTOMERS_DB:
                raise ValueError(f"Customer {customer_id} not found")

            # Simulate database delay
            await asyncio.sleep(0.1)
            return CUSTOMERS_DB[customer_id]

        @mcp.resource("customers://recent")
        async def get_recent_customers(limit: int = 10) -> List[Customer]:
            """Retrieve recently active customers."""
            logger.info(f"Retrieving {limit} recent customers")

            # Sort by last interaction, return most recent
            sorted_customers = sorted(
                CUSTOMERS_DB.values(),
                key=lambda c: c.last_interaction or datetime.min,
                reverse=True
            )

            return sorted_customers[:limit]

        # MCP Tool: Create Support Ticket
        @mcp.tool()
        async def create_support_ticket(request: TicketRequest) -> dict:
            """Create a new customer support ticket."""
            logger.info(f"Creating ticket for customer {request.customer_id}")

            # Validate customer exists
            if request.customer_id not in CUSTOMERS_DB:
                raise ValueError(f"Customer {request.customer_id} not found")

            # Simulate ticket creation
            ticket_id = f"TICKET-{datetime.now().strftime('%Y%m%d%H%M%S')}"

            ticket = {
                "ticket_id": ticket_id,
                "customer_id": request.customer_id,
                "subject": request.subject,
                "description": request.description,
                "priority": request.priority,
                "status": "open",
                "created_at": datetime.now().isoformat()
            }

            return ticket

        # MCP Tool: Calculate Account Value
        @mcp.tool()
        async def calculate_account_value(
            customer_id: str,
            purchase_history: List[float]
        ) -> dict:
            """Calculate total account value and average purchase."""
            logger.info(f"Calculating account value for {customer_id}")

            if not purchase_history:
                return {
                    "customer_id": customer_id,
                    "total_value": 0.0,
                    "average_purchase": 0.0,
                    "purchase_count": 0
                }

            total = sum(purchase_history)
            average = total / len(purchase_history)

            return {
                "customer_id": customer_id,
                "total_value": round(total, 2),
                "average_purchase": round(average, 2),
                "purchase_count": len(purchase_history)
            }

        # MCP Prompt: Customer Service Response Template
        @mcp.prompt("customer_service_response")
        async def generate_service_response_prompt(
            customer_name: str,
            issue_type: str,
            resolution_steps: List[str]
        ) -> str:
            """Generate a professional customer service response."""

            steps_text = "\n".join([
                f"{i+1}. {step}"
                for i, step in enumerate(resolution_steps)
            ])

            return f"""
        You are a professional customer service representative.
        Generate a helpful and empathetic response for the customer.

        Customer: {customer_name}
        Issue Type: {issue_type}

        Resolution Steps:
        {steps_text}

        Guidelines:
        - Be professional but warm
        - Acknowledge the customer's concern
        - Provide clear, actionable steps
        - End with an offer for further assistance
        - Keep the tone positive and solution-focused

        Generate a complete customer service response
        following these guidelines.
        """

        async def main():
            """Main entry point for the MCP server."""
            print("üöÄ Starting Customer Service MCP Server...")
            print("üìã Available Resources:")
            print("   - customer://{customer_id} - Get customer info")
            print("   - customers://recent - Get recent customers")
            print("üîß Available Tools:")
            print("   - create_support_ticket - Create support ticket")
            print("   - calculate_account_value - Calculate account value")
            print("üìù Available Prompts:")
            print("   - customer_service_response - Generate responses")
            print("\n‚úÖ Server ready for connections!")

            # Run the server
            await mcp.run()

        if __name__ == "__main__":
            asyncio.run(main())
      metadata:
        extension: ".py"
        language: "python"

    "src/openai_integration.py":
      content: |
        """OpenAI integration with MCP server."""
        
        import json
        import asyncio
        from contextlib import AsyncExitStack
        from openai import AsyncOpenAI
        from mcp import ClientSession, StdioServerParameters
        from mcp.client.stdio import stdio_client
        from typing import List, Dict, Any
        from config import Config

        class OpenAI_MCP_ChatBot:
            def __init__(self, api_key: str):
                self.client = AsyncOpenAI(api_key=api_key)
                self.sessions = []
                self.exit_stack = AsyncExitStack()
                self.available_tools = []
                self.tool_to_session = {}

            async def connect_to_server(self,
                                       server_name: str,
                                       server_config: dict) -> None:
                """Connect to a single MCP server."""
                try:
                    server_params = StdioServerParameters(**server_config)
                    stdio_transport = await self.exit_stack.enter_async_context(
                        stdio_client(server_params)
                    )
                    read, write = stdio_transport
                    session = await self.exit_stack.enter_async_context(
                        ClientSession(read, write)
                    )
                    await session.initialize()
                    self.sessions.append(session)

                    # List available tools for this session
                    response = await session.list_tools()
                    tools = response.tools
                    print(f"Connected to {server_name} with tools:",
                          [t.name for t in tools])

                    for tool in tools:
                        self.tool_to_session[tool.name] = session
                        # Convert MCP tool to OpenAI tool format
                        openai_tool = {
                            "type": "function",
                            "function": {
                                "name": tool.name,
                                "description": tool.description,
                                "parameters": tool.inputSchema
                            }
                        }
                        self.available_tools.append(openai_tool)
                except Exception as e:
                    print(f"Failed to connect to {server_name}: {e}")

            async def connect_to_servers(self):
                """Connect to all configured MCP servers."""
                try:
                    with open("server_config.json", "r") as file:
                        data = json.load(file)

                    servers = data.get("mcpServers", {})
                    for server_name, server_config in servers.items():
                        await self.connect_to_server(server_name, server_config)
                except Exception as e:
                    print(f"Error loading server configuration: {e}")
                    raise

            async def process_query(self, query: str):
                """Process a query using OpenAI with MCP tools."""
                messages = [{"role": "user", "content": query}]

                response = await self.client.chat.completions.create(
                    model=Config.OPENAI_MODEL,
                    messages=messages,
                    tools=self.available_tools if self.available_tools else None
                )

                process_query = True
                while process_query:
                    message = response.choices[0].message

                    if message.content:
                        print(message.content)

                    # Handle tool calls
                    if message.tool_calls:
                        messages.append({
                            "role": "assistant",
                            "content": message.content,
                            "tool_calls": message.tool_calls
                        })

                        for tool_call in message.tool_calls:
                            tool_name = tool_call.function.name
                            tool_args = json.loads(tool_call.function.arguments)

                            print(f"Calling tool {tool_name} with args {tool_args}")

                            # Use the correct session for this tool
                            session = self.tool_to_session[tool_name]
                            result = await session.call_tool(
                                tool_name,
                                arguments=tool_args
                            )

                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": str(result.content)
                            })

                        # Get the next response
                        response = await self.client.chat.completions.create(
                            model=Config.OPENAI_MODEL,
                            messages=messages,
                            tools=self.available_tools if self.available_tools else None
                        )
                    else:
                        process_query = False

            async def chat_loop(self):
                """Run an interactive chat loop"""
                print("\nOpenAI MCP Chatbot Started!")
                print("Type your queries or 'quit' to exit.")

                while True:
                    try:
                        query = input("\nQuery: ").strip()
                        if query.lower() == 'quit':
                            break
                        await self.process_query(query)
                    except Exception as e:
                        print(f"\nError: {str(e)}")

            async def cleanup(self):
                """Cleanly close all resources."""
                await self.exit_stack.aclose()

        # Usage example
        async def main():
            Config.validate()
            if Config.LLM_PROVIDER != "openai":
                print("This example requires LLM_PROVIDER=openai in .env")
                return
                
            chatbot = OpenAI_MCP_ChatBot(api_key=Config.OPENAI_API_KEY)
            try:
                await chatbot.connect_to_servers()
                await chatbot.chat_loop()
            finally:
                await chatbot.cleanup()

        if __name__ == "__main__":
            asyncio.run(main())
      metadata:
        extension: ".py"
        language: "python"

    "src/anthropic_integration.py":
      content: |
        """Anthropic integration with MCP server."""
        
        import json
        import asyncio
        from contextlib import AsyncExitStack
        from anthropic import Anthropic
        from mcp import ClientSession, StdioServerParameters
        from mcp.client.stdio import stdio_client
        from typing import List, Dict, Any
        from config import Config

        class Anthropic_MCP_ChatBot:
            def __init__(self, api_key: str):
                self.anthropic = Anthropic(api_key=api_key)
                self.sessions = []
                self.exit_stack = AsyncExitStack()
                self.available_tools = []
                self.tool_to_session = {}

            async def connect_to_server(self,
                                       server_name: str,
                                       server_config: dict) -> None:
                """Connect to a single MCP server."""
                try:
                    server_params = StdioServerParameters(**server_config)
                    stdio_transport = await self.exit_stack.enter_async_context(
                        stdio_client(server_params)
                    )
                    read, write = stdio_transport
                    session = await self.exit_stack.enter_async_context(
                        ClientSession(read, write)
                    )
                    await session.initialize()
                    self.sessions.append(session)

                    # List available tools for this session
                    response = await session.list_tools()
                    tools = response.tools
                    print(f"Connected to {server_name} with tools:",
                          [t.name for t in tools])

                    for tool in tools:
                        self.tool_to_session[tool.name] = session
                        self.available_tools.append({
                            "name": tool.name,
                            "description": tool.description,
                            "input_schema": tool.inputSchema
                        })
                except Exception as e:
                    print(f"Failed to connect to {server_name}: {e}")

            async def connect_to_servers(self):
                """Connect to all configured MCP servers."""
                try:
                    with open("server_config.json", "r") as file:
                        data = json.load(file)

                    servers = data.get("mcpServers", {})
                    for server_name, server_config in servers.items():
                        await self.connect_to_server(server_name, server_config)
                except Exception as e:
                    print(f"Error loading server configuration: {e}")
                    raise

            async def process_query(self, query: str):
                """Process a query using Claude with MCP tools."""
                messages = [{'role': 'user', 'content': query}]

                response = self.anthropic.messages.create(
                    max_tokens=2024,
                    model=Config.ANTHROPIC_MODEL,
                    tools=self.available_tools,
                    messages=messages
                )

                process_query = True
                while process_query:
                    assistant_content = []
                    for content in response.content:
                        if content.type == 'text':
                            print(content.text)
                            assistant_content.append(content)
                            if len(response.content) == 1:
                                process_query = False

                        elif content.type == 'tool_use':
                            assistant_content.append(content)
                            messages.append({
                                'role': 'assistant',
                                'content': assistant_content
                            })

                            tool_id = content.id
                            tool_args = content.input
                            tool_name = content.name

                            print(f"Calling tool {tool_name} with args {tool_args}")

                            # Use the correct session for this tool
                            session = self.tool_to_session[tool_name]
                            result = await session.call_tool(
                                tool_name,
                                arguments=tool_args
                            )

                            messages.append({
                                "role": "user",
                                "content": [{
                                    "type": "tool_result",
                                    "tool_use_id": tool_id,
                                    "content": result.content
                                }]
                            })

                            response = self.anthropic.messages.create(
                                max_tokens=2024,
                                model=Config.ANTHROPIC_MODEL,
                                tools=self.available_tools,
                                messages=messages
                            )

                            if (len(response.content) == 1 and
                                response.content[0].type == "text"):
                                print(response.content[0].text)
                                process_query = False

            async def chat_loop(self):
                """Run an interactive chat loop"""
                print("\nAnthropic MCP Chatbot Started!")
                print("Type your queries or 'quit' to exit.")

                while True:
                    try:
                        query = input("\nQuery: ").strip()
                        if query.lower() == 'quit':
                            break
                        await self.process_query(query)
                    except Exception as e:
                        print(f"\nError: {str(e)}")

            async def cleanup(self):
                """Cleanly close all resources."""
                await self.exit_stack.aclose()

        # Usage example
        async def main():
            Config.validate()
            if Config.LLM_PROVIDER != "anthropic":
                print("This example requires LLM_PROVIDER=anthropic in .env")
                return
                
            chatbot = Anthropic_MCP_ChatBot(api_key=Config.ANTHROPIC_API_KEY)
            try:
                await chatbot.connect_to_servers()
                await chatbot.chat_loop()
            finally:
                await chatbot.cleanup()

        if __name__ == "__main__":
            asyncio.run(main())
      metadata:
        extension: ".py"
        language: "python"

    "src/langchain_integration.py":
      content: |
        """LangChain integration with MCP server."""
        
        import asyncio
        from langchain_mcp_adapters.client import MultiServerMCPClient
        from langgraph.prebuilt import create_react_agent
        from langchain_openai import ChatOpenAI
        from config import Config

        async def setup_langchain_mcp_agent():
            """Set up a LangChain agent with MCP tools."""

            # Initialize the language model
            llm = ChatOpenAI(
                model=Config.OPENAI_MODEL,
                temperature=0.1,
                api_key=Config.OPENAI_API_KEY
            )

            # Connect to our MCP server using MultiServerMCPClient
            client = MultiServerMCPClient({
                "customer-service": {
                    "command": "poetry",
                    "args": ["run", "python", "src/main.py"],
                    "transport": "stdio"
                }
            })

            # Get all available tools from MCP servers
            tools = await client.get_tools()

            # Create a ReAct agent with the tools
            agent = create_react_agent(llm, tools)

            return agent, client

        async def run_customer_service_scenarios():
            """Demonstrate LangChain + MCP integration."""
            print("üîó Setting up LangChain + MCP integration...")

            agent, client = await setup_langchain_mcp_agent()

            # Example customer service scenarios
            scenarios = [
                "Look up customer 12345 and summarize their account status",
                "Create a high-priority support ticket for customer 67890 about billing",
                "Calculate account value for customer with purchases: $150, $300, $89"
            ]

            for scenario in scenarios:
                print(f"\nüìû Scenario: {scenario}")
                try:
                    response = await agent.ainvoke({
                        "messages": [{"role": "user", "content": scenario}]
                    })
                    print(f"ü§ñ Response: {response}")
                except Exception as e:
                    print(f"‚ùå Error: {e}")

                print("-" * 60)

        async def main():
            """Main entry point."""
            Config.validate()
            if Config.LLM_PROVIDER != "openai":
                print("LangChain example requires OpenAI. Set LLM_PROVIDER=openai in .env")
                return
                
            await run_customer_service_scenarios()

        if __name__ == "__main__":
            asyncio.run(main())
      metadata:
        extension: ".py"
        language: "python"

    "src/dspy_integration.py":
      content: |
        """DSPy integration with MCP server."""
        
        import asyncio
        import dspy
        from mcp import ClientSession, StdioServerParameters
        from mcp.client.stdio import stdio_client
        from config import Config

        # Define a DSPy signature for our customer service tasks
        class CustomerServiceSignature(dspy.Signature):
            """Handle customer service requests using available tools."""
            request: str = dspy.InputField(desc="Customer service request")
            response: str = dspy.OutputField(desc="Helpful customer service response")

        async def setup_dspy_mcp_integration():
            """Set up DSPy with MCP tools."""
            
            # Configure DSPy with your preferred language model
            if Config.LLM_PROVIDER == "openai":
                llm = dspy.LM(f"openai/{Config.OPENAI_MODEL}", api_key=Config.OPENAI_API_KEY)
            elif Config.LLM_PROVIDER == "anthropic":
                llm = dspy.LM(f"anthropic/{Config.ANTHROPIC_MODEL}", api_key=Config.ANTHROPIC_API_KEY)
            else:
                print("DSPy requires OpenAI or Anthropic provider")
                return None
                
            dspy.configure(lm=llm)

            # Create MCP client connection
            server_params = StdioServerParameters(
                command="poetry",
                args=["run", "python", "src/main.py"]
            )

            async with stdio_client(server_params) as (read, write):
                async with ClientSession(read, write) as session:
                    # Initialize the MCP connection
                    await session.initialize()

                    # List available tools
                    tools = await session.list_tools()

                    # Convert MCP tools to DSPy tools
                    dspy_tools = []
                    for tool in tools.tools:
                        dspy_tools.append(dspy.Tool.from_mcp_tool(session, tool))

                    # Create a ReAct agent with the tools
                    react = dspy.ReAct(CustomerServiceSignature, tools=dspy_tools)

                    # Test the integration
                    result = await react.acall(
                        request="Look up customer 12345 and create a support ticket"
                    )

                    print(f"DSPy Result: {result}")

        async def main():
            """Main entry point."""
            Config.validate()
            await setup_dspy_mcp_integration()

        if __name__ == "__main__":
            asyncio.run(main())
      metadata:
        extension: ".py"
        language: "python"

    "src/litellm_integration.py":
      content: |
        """LiteLLM integration with MCP server."""
        
        import asyncio
        import os
        import litellm
        from mcp import ClientSession, StdioServerParameters
        from mcp.client.stdio import stdio_client
        from litellm import experimental_mcp_client
        from config import Config

        async def setup_litellm_mcp():
            """Set up LiteLLM with MCP tools."""

            # Create MCP server connection
            server_params = StdioServerParameters(
                command="poetry",
                args=["run", "python", "src/main.py"]
            )

            async with stdio_client(server_params) as (read, write):
                async with ClientSession(read, write) as session:
                    # Initialize the MCP connection
                    await session.initialize()

                    # Load MCP tools in OpenAI format
                    tools = await experimental_mcp_client.load_mcp_tools(
                        session=session,
                        format="openai"
                    )

                    print(f"Loaded {len(tools)} MCP tools")

                    # Use tools with different models
                    models_to_test = []
                    
                    if Config.LLM_PROVIDER == "openai":
                        models_to_test.append(Config.OPENAI_MODEL)
                    elif Config.LLM_PROVIDER == "anthropic":
                        models_to_test.append(Config.ANTHROPIC_MODEL)
                    else:
                        models_to_test = [Config.OPENAI_MODEL, Config.ANTHROPIC_MODEL]

                    for model in models_to_test:
                        try:
                            print(f"\nTesting with {model}...")
                            response = await litellm.acompletion(
                                model=model,
                                messages=[{
                                    "role": "user",
                                    "content": "Get customer 12345 information"
                                }],
                                tools=tools
                            )
                            print(f"{model} response: {response}")
                        except Exception as e:
                            print(f"Error with {model}: {e}")

        async def main():
            """Main entry point."""
            Config.validate()
            await setup_litellm_mcp()

        if __name__ == "__main__":
            asyncio.run(main())
      metadata:
        extension: ".py"
        language: "python"

    "tests/test_mcp_server.py":
      content: |
        """Tests for MCP Customer Service Server."""
        
        import pytest
        import asyncio
        from datetime import datetime
        from src.main import (
            Customer,
            TicketRequest,
            get_customer_info,
            create_support_ticket,
            calculate_account_value,
            generate_service_response_prompt
        )

        @pytest.mark.asyncio
        async def test_get_customer_info():
            """Test retrieving customer information."""
            # Test valid customer
            customer = await get_customer_info("12345")
            assert customer.id == "12345"
            assert customer.name == "Alice Johnson"
            assert customer.email == "alice@example.com"
            
            # Test invalid customer
            with pytest.raises(ValueError):
                await get_customer_info("99999")

        @pytest.mark.asyncio
        async def test_create_support_ticket():
            """Test creating a support ticket."""
            request = TicketRequest(
                customer_id="12345",
                subject="Billing Issue",
                description="I was charged twice",
                priority="high"
            )
            
            ticket = await create_support_ticket(request)
            assert ticket["customer_id"] == "12345"
            assert ticket["subject"] == "Billing Issue"
            assert ticket["priority"] == "high"
            assert ticket["status"] == "open"
            assert "TICKET-" in ticket["ticket_id"]

        @pytest.mark.asyncio
        async def test_calculate_account_value():
            """Test calculating account value."""
            # Test with purchases
            result = await calculate_account_value(
                "12345",
                [100.0, 250.0, 75.0]
            )
            assert result["total_value"] == 425.0
            assert result["average_purchase"] == 141.67
            assert result["purchase_count"] == 3
            
            # Test with no purchases
            result = await calculate_account_value("12345", [])
            assert result["total_value"] == 0.0
            assert result["average_purchase"] == 0.0
            assert result["purchase_count"] == 0

        @pytest.mark.asyncio
        async def test_generate_service_response_prompt():
            """Test generating service response prompt."""
            prompt = await generate_service_response_prompt(
                "Alice Johnson",
                "Account Access",
                ["Reset your password", "Clear browser cache", "Try logging in again"]
            )
            
            assert "Alice Johnson" in prompt
            assert "Account Access" in prompt
            assert "1. Reset your password" in prompt
            assert "2. Clear browser cache" in prompt
            assert "3. Try logging in again" in prompt

        def test_customer_model_validation():
            """Test Customer model validation."""
            # Valid customer
            customer = Customer(
                id="123",
                name="Test User",
                email="test@example.com"
            )
            assert customer.email == "test@example.com"
            
            # Invalid email
            with pytest.raises(ValueError):
                Customer(
                    id="123",
                    name="Test User",
                    email="invalid-email"
                )

        def test_ticket_request_validation():
            """Test TicketRequest model validation."""
            # Valid priority
            request = TicketRequest(
                customer_id="123",
                subject="Test",
                description="Test description",
                priority="urgent"
            )
            assert request.priority == "urgent"
            
            # Invalid priority
            with pytest.raises(ValueError):
                TicketRequest(
                    customer_id="123",
                    subject="Test",
                    description="Test description",
                    priority="invalid"
                )
      metadata:
        extension: ".py"
        language: "python"

    ".gitignore":
      content: |
        # Python
        __pycache__/
        *.py[cod]
        *$py.class
        *.so
        .Python
        build/
        develop-eggs/
        dist/
        downloads/
        eggs/
        .eggs/
        lib/
        lib64/
        parts/
        sdist/
        var/
        wheels/
        *.egg-info/
        .installed.cfg
        *.egg
        MANIFEST

        # Virtual Environment
        .venv/
        venv/
        ENV/
        env/

        # Poetry
        poetry.lock

        # IDE
        .vscode/
        .idea/
        *.swp
        *.swo
        *~

        # Environment variables
        .env
        .env.local
        .env.*.local

        # Testing
        .pytest_cache/
        .coverage
        htmlcov/
        .tox/
        .hypothesis/

        # Misc
        .DS_Store
        *.log
        .ruff_cache/
      metadata:
        extension: ""
        language: "gitignore"